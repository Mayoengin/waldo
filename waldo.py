# -*- coding: utf-8 -*-
"""waldo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_R_SKPzwf8qpqH51j2P32hvMKG1O83Qo

# **Import libraries**
"""

import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from PIL import Image
import numpy as np
import os
import torchvision
from torchvision import transforms
from sklearn.model_selection import train_test_split
import math
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import torch.optim
from tqdm import tqdm
import random
from torch.linalg import vector_norm

# Set seed for reproducibility (produce same random seed every time the code is run)
SEED = 123

# Sets the seed for Python built-in random number generator
random.seed(SEED)

# Sets the seed for numpy library random number generator
np.random.seed(SEED)

# Sets the seed for pytorch random number generator in CPU
torch.manual_seed(SEED)

# Sets the seed for pytorch random number generator in GPU
torch.cuda.manual_seed_all(SEED)

"""# **Set up GPU**
You should first *Change runtime type* of the Colab notbook to enable the use of GPU.
"""

if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f'There are {torch.cuda.device_count()} GPU(s) available.')
    print('Device name:', torch.cuda.get_device_name(0))

else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

"""# **Initialize hyperparameters**"""

batch_size = 100
num_epochs = 30
num_workers = 1

# Mount google drive
from google.colab import drive
drive.mount('/content/drive')

# Define dataset directory
# Change your directory in case of not using google colab
data_dir = '/content/Hey-Waldo'

# Define path to save model's state file
model_state_path = '/content/drive/MyDrive/'

"""# **Part 1: Data Preparation**

## 1.1 Load dataset
"""

!git clone https://github.com/vc1492a/Hey-Waldo.git

"""## 1.2 Load and preprocess data
Here the images in folder 64 will be resized to 128*128 and then all images and their labels will be converted to a numpy array.
"""

# Create a function to load images from different folders, resize them if it is required and create labels
def preprocess_images(data_dir):
    all_images = []
    all_labels = []

    # Only use folder 128 and 64
    for folder in ['128', '64']:
        for subfolder in ['waldo', 'notwaldo']:
            folder_path = os.path.join(data_dir, folder, subfolder)
            # Assign label 1 to waldo and 0 to notwaldo
            label = 1 if subfolder == 'waldo' else 0
            # Set resize to true only if it is in folder 64
            resize = True if folder == '64' else False

            for img_name in os.listdir(folder_path):
                img_path = os.path.join(folder_path, img_name)
                img = Image.open(img_path)

                if resize:
                  img = img.resize((128, 128))
                img = np.array(img)
                all_images.append(img)
                all_labels.append(label)

    return all_images, np.array(all_labels)

"""## 1.3 Split the dataset to train/validation/test sets"""

# Load images and labels and preprocess them
all_images, all_labels = preprocess_images(data_dir)

# Split data: 90% for training and 10% for testing & 10% of training for validation
# random_state ensures reproducability, shuffle ensures randomization
# stratify ensures being representative of the entire dataset (having the same proportion of different classes in splitting)
train_images, test_images, train_labels, test_labels = train_test_split(all_images, all_labels, test_size=0.1, random_state=42, shuffle=True, stratify=all_labels)
train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.1, random_state=42, shuffle=True, stratify=train_labels)

print(f"Training size: {len(train_images)}")
print(f"Training size Waldo: {np.sum(train_labels == 1)}")
print(f"Training size notWaldo: {np.sum(train_labels == 0)}")
print(f"Validation size: {len(val_images)}")
print(f"Test size: {len(test_images)}")

"""The result shows the training set is quite unbalanced. In addition number of training samples without waldo is very high. Therefore, a new training images will be made. We tried two different solutions:


1.   We randomely sample an amount of train images without waldo (downsampling notWaldo) and also use data augmentation methods to create more samples with waldo (upsampling waldo).
2.   We only upsample waldo images without downsampling notWaldo.

 In the end, new created samples and the original samples are combined to create a new training set.

 Based on our observation the second method gives better training results.
"""

# diff: upsampling waldo


# Define number of images for waldo
num_waldo = 5100 # in total will be +53 (already existant waldo samples)
num_duplicate_waldo = math.floor(num_waldo / np.sum(train_labels == 1))

# Separate images with and without Waldo in the training set
waldo_images = [img for img, label in zip(train_images, train_labels) if label == 1]
not_waldo_images = [img for img, label in zip(train_images, train_labels) if label == 0]

# Convert numpy arrays to PIL Images for applying transformation
waldo_images_pil = [Image.fromarray(img) for img in waldo_images]

# Duplicate Waldo images and apply augmentation
aug_waldo_images = []
portion = num_duplicate_waldo // 5 # 5 is number of augmentation method

for img in waldo_images_pil:
    for i in range(num_duplicate_waldo):
        if i < portion * 1:
            transform = transforms.RandomHorizontalFlip(p=1.0)
        elif i < portion * 2:
            transform = transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=1.0)
        elif i < portion * 3:
            transform = transforms.RandomGrayscale(p=1.0)
        elif i < portion * 4:
            transform = transforms.RandomRotation(15)
        else:
            aug_transform = transforms.RandomVerticalFlip(p=1.0)

        aug_img = transform(img)
        aug_waldo_images.append(aug_img)


# Combining the sampled non-Waldo images with all Waldo images
final_train_images = waldo_images + not_waldo_images + aug_waldo_images
final_train_labels = [1] * len(waldo_images) + [0] * len(not_waldo_images) + [1] * len(aug_waldo_images)

# Combine and shuffle the final training set
combined_set = list(zip(final_train_images, final_train_labels))
random.shuffle(combined_set)
final_train_images, final_train_labels = zip(*combined_set)


# Convert PIL Image to a numPy array (because of warnings)
train_images_np = [np.array(img) for img in final_train_images]

# Convert the list of numPy arrays to a single NumPy array (because of warnings)
train_images = np.array(train_images_np)
train_labels = np.array(final_train_labels)

# Print final dataset sizes
print(f"Training size: {len(train_images)}")
print(f"Training size Waldo: {np.sum(train_labels == 1)}")
print(f"Training size notWaldo: {np.sum(train_labels == 0)}")
print(f"Validation size: {len(val_images)}")
print(f"Test size: {len(test_images)}")

# # diff: upsampling waldo + downsampling notwaldo


# # Define number of images for waldo and not waldo
# num_not_waldo = 3079 #3000
# num_waldo = 3000 # in total will be +53 (already existant waldo samples)
# num_duplicate_waldo = math.floor(num_waldo / np.sum(train_labels == 1))

# # Separate images with and without Waldo in the training set
# waldo_images = [img for img, label in zip(train_images, train_labels) if label == 1]
# not_waldo_images = [img for img, label in zip(train_images, train_labels) if label == 0]

# # Sampling not-Waldo images
# sampled_not_waldo_images = random.sample(not_waldo_images, num_not_waldo)

# # Convert numpy arrays to PIL Images for applying transformation
# waldo_images_pil = [Image.fromarray(img) for img in waldo_images]

# # Duplicate Waldo images and apply augmentation
# aug_waldo_images = []
# portion = num_duplicate_waldo // 5 # 5 is number of augmentation method

# for img in waldo_images_pil:
#     for i in range(num_duplicate_waldo):
#         if i < portion * 1:
#             transform = transforms.RandomHorizontalFlip(p=1.0)
#         elif i < portion * 2:
#             transform = transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=1.0)
#         elif i < portion * 3:
#             transform = transforms.RandomGrayscale(p=1.0)
#         elif i < portion * 4:
#             transform = transforms.RandomRotation(15)
#         else:
#             aug_transform = transforms.RandomVerticalFlip(p=1.0)

#         aug_img = transform(img)
#         aug_waldo_images.append(aug_img)


# # Combining the sampled non-Waldo images with all Waldo images
# final_train_images = waldo_images + sampled_not_waldo_images + aug_waldo_images
# final_train_labels = [1] * len(waldo_images) + [0] * len(sampled_not_waldo_images) + [1] * len(aug_waldo_images)

# # Combine and shuffle the final training set
# combined_set = list(zip(final_train_images, final_train_labels))
# random.shuffle(combined_set)
# final_train_images, final_train_labels = zip(*combined_set)


# # Convert PIL Image to a numPy array (because of warnings)
# train_images_np = [np.array(img) for img in final_train_images]

# # Convert the list of numPy arrays to a single NumPy array (because of warnings)
# train_images = np.array(train_images_np)
# train_labels = np.array(final_train_labels)

# # Print final dataset sizes
# print(f"Training size: {len(train_images)}")
# print(f"Training size Waldo: {np.sum(train_labels == 1)}")
# print(f"Training size notWaldo: {np.sum(train_labels == 0)}")
# print(f"Validation size: {len(val_images)}")
# print(f"Test size: {len(test_images)}")

"""# **Part 2: Train the model using a triplet loss**

## 2.1 Triplet Dataset Preparation

### 2.1.1 Create TripletDataset class
"""

# Create custom dataset class
# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html
# https://www.kaggle.com/code/hirotaka0122/triplet-loss-with-pytorch
# https://www.kaggle.com/code/sapthrishi007/shopee-pytorch-siamese-triplet-loss-xlmroberta/notebook?scriptVersionId=59308069

class TripletDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform
        self.to_pil = transforms.ToPILImage()

        self.triplets = self.create_triplets(images, labels)

    # Create a function to create triplets (anchor, positive, negative)
    # anchore is the selected patch
    def create_triplets(self, images, labels):
        triplets = []
        for idx, (img, label) in enumerate(zip(images, labels)):  # zip to create tuple of (image, label)
            # Create positive indices for the images with the same label but not the same index
            positive_idxs = [i for i, l in enumerate(labels) if l == label and i != idx]
            # Create negative indices for the images with the oposite label
            negative_idxs = [i for i, l in enumerate(labels) if l != label]
            positive_idx = np.random.choice(positive_idxs)
            negative_idx = np.random.choice(negative_idxs)
            triplets.append((img, images[positive_idx], images[negative_idx]))
        return triplets

    def __len__(self):
        return len(self.triplets)

    def __getitem__(self, idx):
        anchor, positive, negative = self.triplets[idx]
        if self.transform:
           anchor = self.transform(self.to_pil(anchor))
           positive = self.transform(self.to_pil(positive))
           negative = self.transform(self.to_pil(negative))
           label = self.labels[idx]
           return anchor, positive, negative, label

"""### 2.1.2 Calculate Mean and STD"""

# # Create a function to calculate mean and std to use in transform.Normalize
# https://discuss.pytorch.org/t/about-normalization-using-pre-trained-vgg16-networks/23560/6?u=kuzand
def calculate_mean_std(loader):
    mean, std, num_batches = 0, 0, 0

    for img, _, _, _ in loader:
        batch_size = img.size(0)
        # Reshape the image from [B, C, W, H] to [B, C, W * H] Batch, Channel, Width, Height
        # keep the first and second element at their position but flatten the rest
        # W*H is the total number of pixels in each channel of an image
        img = img.view(img.size(0), img.size(1), -1)
        # Get the mean of each image and sum them over batch
        mean += img.mean(2).sum(0)
        # Get the std of each image and sum them over batch
        std += img.std(2).sum(0)
        # Add the number of images in one batch (batch_size) to number of batches
        num_batches += batch_size

    mean /= num_batches
    std /= num_batches
    return mean, std

# Create a temporary dataset to calculate mean and std of training images
temp_dataset = TripletDataset(train_images, train_labels, transform=transforms.ToTensor())
temp_loader = DataLoader(temp_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

# Calculate mean and std
# calculation of mean and std is only based on training data but it is applied on all datasets
mean, std = calculate_mean_std(temp_loader)

print(f"Mean of training set: {mean}")
print(f"STD of training set: {std}")

"""### 2.1.3 Create Datasets and DataLoader"""

# Transformations for data augmentation
transform_train = transforms.Compose([transforms.RandomRotation(10),
                                      transforms.RandomHorizontalFlip(),
                                      transforms.ToTensor(),
                                      transforms.Normalize(mean, std)
                                      ])

transform_test_val = transforms.Compose([transforms.ToTensor(),
                                         transforms.Normalize(mean, std)
                                        ])

# Create datasets
train_set = TripletDataset(train_images, train_labels, transform=transform_train)
val_set = TripletDataset(val_images, val_labels, transform=transform_test_val)
test_set = TripletDataset(test_images, test_labels, transform=transform_test_val)

# Create DataLoaders
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)
val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)
test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)

"""### 2.1.4 Visualize DataLoader"""

# Convert from tensor to numpy
mean = mean.numpy()
std = std.numpy()

# Create a function to visualize some of the images
# WPO8 TL
def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated

# Get a batch of training data and visualize it
for batch in iter(train_loader):
    # Unpack the triplets
    anchors, positives, negatives, _ = batch
    imshow(torchvision.utils.make_grid(anchors))
    break  # only display the first batch

# Get a batch of training data
for batch in iter(train_loader):
    # Unpack the triplets
    anchors, positives, negatives, label = batch
    print(f"anchore image shape: {anchors.shape}")     # (B, C, H, W)
    print(f"positive image shape: {positives.shape}")  # (B, C, H, W)
    print(f"negative image shape: {negatives.shape}")  # (B, C, H, W)
    print(f"label shape: {label.shape}") # (B)
    break  # only display the first batch

# Get the labels for the batch
print(f"labels for the first 10 loaded images: {label[:10]}")

# Visualize dataloader with the distribution of different classes
# https://towardsdatascience.com/demystifying-pytorchs-weightedrandomsampler-by-example-a68aceccb452

def visualize_dataloader(dataLoader, label=None, with_outputs=True):
    total_num_images = len(dataLoader.dataset)
    class_0_batch_counts = []
    class_1_batch_counts = []

    for i, batch in enumerate(dataLoader):
        classes = batch[3]  # labels are in the index=3 (anchor, positive, negative, label)

        class_ids, class_counts = classes.unique(return_counts=True)
        class_ids = set(class_ids.tolist())
        class_counts = class_counts.tolist()

        if len(class_ids) == 2:
            class_0_batch_counts.append(class_counts[0])
            class_1_batch_counts.append(class_counts[1])
        elif len(class_ids) == 1 and 0 in class_ids:
            class_0_batch_counts.append(class_counts[0])
            class_1_batch_counts.append(0)
        elif len(class_ids) == 1 and 1 in class_ids:
            class_0_batch_counts.append(0)
            class_1_batch_counts.append(class_counts[0])

    if with_outputs:
        fig, ax = plt.subplots(1, figsize=(20, 6))

        ind = np.arange(len(class_0_batch_counts))
        width = 0.35

        ax.bar(
            ind,
            class_0_batch_counts,
            width,
            label=(label[0] if label is not None else "0"),
        )
        ax.bar(
            ind + width,
            class_1_batch_counts,
            width,
            label=(label[1] if label is not None else "1"),
        )
        ax.set_xticks(ind, ind + 1)
        ax.set_xlabel("Batch index", fontsize=12)
        ax.set_ylabel("No. of images in batch", fontsize=12)
        #ax.set_aspect("equal")
        fig.tight_layout()

        plt.legend()
        plt.show()

        print(
            f'Avg Proportion of {(label[0] if label is not None else "Class 0")} per batch: {round((np.array(class_0_batch_counts) / 10).mean(), 3)}'
        )
        print(
            f'Avg Proportion of {(label[1] if label is not None else "Class 1")} per batch: {round((np.array(class_1_batch_counts) / 10).mean(), 3)}'
        )

    return class_0_batch_counts, class_1_batch_counts #, idxs_seen

# Visualize dataloader distribution per batch
class_0_batch_counts, class_1_batch_counts = visualize_dataloader(train_loader, {0: "NotWaldo", 1: "Waldo"})

"""### 2.1.5 Balance the dataset

The goal of this part is to make the dataset balanced by using *WeightedRandomSampler* but since we upsampeled waldo images in a way to have balanced dataset, there is no need to apply this method anymore.
"""

# diff: by the augmentation method we used before we already have a balanced dataset, so this one is not needed


# Calculate weights of each class
# https://www.maskaravivek.com/post/pytorch-weighted-random-sampler/
# https://towardsdatascience.com/demystifying-pytorchs-weightedrandomsampler-by-example-a68aceccb452
# https://discuss.pytorch.org/t/how-to-use-weightedrandomsampler-for-imbalanced-data/110578

# # Count total number of each class
# class_count = np.unique(train_labels, return_counts=True)[1]
# weight = 1. / class_count
# # Create a numpy array with weights for each label
# samples_weight = weight[train_labels]

# # Convert to tensor
# samples_weight = torch.from_numpy(samples_weight)
# # Convert to double
# samples_weigth = samples_weight.double()

# # Create sampler to use in dataloader
# # WeightedRandomSampler randomly selects sample from the dataset with probability of its weight
# # so it insures that the distribution of both classes per batch is balanced
# sampler = WeightedRandomSampler(samples_weight, len(samples_weight))
# train_loader = DataLoader(train_set, batch_size=batch_size, sampler=sampler, num_workers=num_workers)

# # Visualize dataloader distribution per batch
# class_0_batch_counts, class_1_batch_counts, idxs_seen = visualize_dataloader(train_loader, {0: "without Waldo", 1: "with Waldo"})

"""## 2.2 Vision Transformer Model Implementation"""

# Define the patchification process
# WPO7
class Patchify(nn.Module):

    def __init__(self, img_size, patch_size, embed_dim):
        super().__init__()
        self.num_patches = (img_size // patch_size) * (img_size // patch_size)  # height * width
        self.img_size = img_size
        self.patch_size = patch_size
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_channels = 3,  # RGB
                              out_channels = embed_dim,
                              kernel_size = patch_size,
                              stride = patch_size)  # no overlap between patches

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x)   # (batch_size, dim, img_size // patch_size, img_size // patch_size)
        # the input to the transformer should be of shape (batch_size, num_patches, embedding dim)
        # (batch_size, dim, img_size // patch_size, img_size // patch_size) --> flattten --> (batch_size, dim, num_patches) --> tranpose --> (batch_size, num_patches, dim)
        # faltten(2) means return the dimension 0 and 1 the same but from dimension 2 multiply all other dimensions: img_size // patch_size * img_size // patch_size = num_patches
        x = x.flatten(2).transpose(1,2)
        return x

# Define the Feed Forward Layer of the Transformer Encoder
# WPO7
class FeedForward(nn.Module):
    def __init__(self, dim, dropout ):
        super().__init__()

        self.net = nn.Sequential(
            # Normalize input across the features for each sample in a mini-batch.
            # useful for stabilizing the learning process.
            # dim: dimension of patch embeddings.
            nn.LayerNorm(dim),
            # Multiplying input with a weight matrix and adding a bias in a fully connected layer.
            # Transforms the input with dim to dim*4 (allows network to learn more features)
            nn.Linear(dim, dim * 4),
            # Introduce non linearity to the model. outputs the input directly if it's positive, otherwise 0.
            nn.ReLU(),
            # Randomely ignores selected neurons during training based on the value dropout (the probability of a neuron being dropped out).
            # Regularization technique by preventing the model from becoming too reliant on any single neuron.
            nn.Dropout(dropout),
            # dim * 4 --> dim features.
            nn.Linear(dim * 4, dim),
            # Add more regularization.
            nn.Dropout(dropout)
        )

    def forward(self, x):
        return self.net(x)

# Define the Multi-Head Attention of the Transformer Encoder
# WPO7
class MultiHeadAttention(nn.Module):

    def __init__(self, dim, heads, dropout):
        super().__init__()
        self.dim = dim
        self.heads = heads

        self.query = nn.Linear(dim, dim)
        self.keys = nn.Linear(dim, dim)
        self.values = nn.Linear(dim, dim)

        self.norm = nn.LayerNorm(dim)
        self.out_proj = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # N: total number of patches
        B, N, C = x.shape

        x = self.norm(x)         # (B, N, C)

        query = self.query(x)    # (B, N, C)
        key = self.keys(x)       # (B, N, C)
        value = self.values(x)   # (B, N, C)

        dim_head = C // self.heads

        # Split (B,N,C) into (B, N, num_heads, dim_head) --> permute --> (B, num_heads, N, dim_head) because each of the heads, should have (N, dim_head)
        # permute(0,2,1,3) change the position of dim 1 and 2
        query = query.reshape(B, N, self.heads, dim_head).permute(0,2,1,3)
        key = key.reshape(B, N, self.heads, dim_head).permute(0,2,1,3)
        value = value.reshape(B, N, self.heads, dim_head).permute(0,2,1,3)

        # Matrix multiplication of (B, num_heads, N, dim_head) with (B, num_heads, dim_head, N) --> (B, num_heads, N, N)
        # key.transpose(-1, -2): -1 is the last dimension (dim_head). -2 is the second to last dimension (N).
        # so (B, num_heads, N, dim_head) --> transpose --> (B, num_heads, dim_head, N)
        attention_scores = torch.matmul(query, key.transpose(-1, -2))

        scale = dim_head ** -0.5   # (1 / sqrt(dim_head))
        attention_scores = attention_scores * scale
        # Softmax: convert a set of raw scores (logits) into probabilities.
        # The output is a tensor of the same shape as the input, but with values in range (0, 1) that sum to 1 along the specified dimension.
        # softmax is applied to the scores to convert them into attention weights.
        # attention weights: probabilities that shows how much attention (focus) the model should put on each element when computing the weighted sum of values.
        # softmax is applied across the last dimension (dim = -1) in standard attention score with shape (B, num_heads, N, N)
        attention_scores = F.softmax(attention_scores, dim = -1) # (B, num_heads, N, N)
        attention_scores = self.dropout(attention_scores)

        # extract the values
        # (B, num_heads, N, N) matmul (B, num_heads, N, dim_head) --> (B, num_heads, N, dim_head)
        out = torch.matmul(attention_scores, value)

        # (B, num_heads, N, dim_head) --> permute --> (B, N, num_heads, dim_head) --> flatten --> (B, N, C)
        out = out.permute(0,2,1,3).flatten(2)
        out = self.out_proj(out)
        return out

# Define the Transformer Encoder
# WPO7
class Transformer(nn.Module):
    def __init__(self, dim, num_layers, heads, dropout):

        super().__init__()

        self.norm = nn.LayerNorm(dim)
        self.layers = nn.ModuleList([])

        for _ in range(num_layers):
            self.layers.append(nn.ModuleList([
                                              MultiHeadAttention(dim, heads = heads, dropout = dropout),
                                              FeedForward(dim, dropout = dropout)]))

    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x

        return self.norm(x)

# Define ViT model by combining all the modules we defined above, into our final ViT model.
# WPO7
class ViT(nn.Module):
    def __init__(self, image_size, patch_size, num_classes, dim, num_layers, heads, dropout):
        '''
        dim: Dimension of patch embeddings.
        num_layers: Number of transformer layers.
        heads: Number of attention heads in the multi-head attention mechanism.
        '''
        super().__init__()

        self.patchify = Patchify(image_size, patch_size, dim)
        # positional encoding provides information about the position of the patches in the input sequence.
        # helps the model to consider the position of each patch when processing the image.
        # nn.Parameter means it's a learnable parameter and is updated during training.
        # (1, self.patchify.num_patches, dim) is the shape of position for each patch.
        self.pos_encoding = nn.Parameter(torch.randn(1, self.patchify.num_patches, dim))
        self.transformer = Transformer(dim, num_layers, heads, dropout = dropout)

    def forward(self, x):
        x = self.patchify(x)         # (batch_size, N, dim)
        x = x + self.pos_encoding    # (batch_size, N, dim)
        x = self.transformer(x)      # (batch_size, N, dim)
        x = x.mean(1)                # (batch_size, dim)
        return x

"""## 2.3 Instantiation of ViT model


"""

# diff: only upsampling waldo without downsampling notwaldo


# Create an instance of ViT model
image_size = 128
patch_size = 8
unique_classes = np.unique(train_loader.dataset.labels)
num_classes = len(unique_classes)
# overfitting --> change dim = 512 to 256
#dim = 512
# got worse so stick to dim = 512
#dim = 256
# this is the best try of dim --> increase weight_decay=0.01 to 0.02
dim = 512

# overfitting --> change num_layers = 3 to 2
#num_layers = 3
# got worse so stick to num_layers=3
#num_layers = 2
# this is the best try of num_layers --> decrease dim=512 to 256
num_layers = 3

heads = 8
# overfitting --> increase dropout = 0.1 to 0.2
#dropout = 0.1
# better but still overfitting --> increase dropout = 0.2 to 0.3 (if doesn't work stick to this)
#dropout = 0.2
# got worse --> stick to dropout=0.2
#dropout = 0.3
# this is the best try of dropout --> decrease num_layers=3 to 2
dropout = 0.125 # previousely 0.2

model_triplet = ViT(image_size = image_size,
            patch_size = patch_size,
            num_classes = num_classes,
            dim = dim,
            num_layers = num_layers,
            heads = heads,
            dropout = dropout).to(device)

# Define optimizer
## Not learning: loss does not decrease --> decrease lr=0.01 to lr=1e-3
#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
## Not learning: loss does not decrease --> change optimizer to AdamW
#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)
# AdamW is most widely used optimizer for Transformers according to https://arxiv.org/pdf/2302.01107.pdf
# Overfitting! train loss decreases but val loss is not that much good, neg dist ar good, pos dist somehow... --> change scheduler from CosineAnnealingLR to ReduceLROnPlateau
# better but still overfitting --> decrease lr from 5e-4 to lr=1e-4 & use ReduceLROnPlateau
#optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)
#################################### much more better (congrats!) but still not generalizes well --> decrease lr from 1e-4 to lr=5e-5 (if didn't work keep this)
#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
# got worse, so stick to the lr=1e-4 and focus on tuning other hyperparameters --> increase dropout from 0.1 to dropout=0.2
#optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)
# this is the best try of adjusting lr --> increase dropout=0.1 to 0.2
# adjust weight_decay=0.01 to 0.02
#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
# got worse so stick to weight_decay=0.01
#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.02)
# this is the best try of weight_decay --> ?
optimizer = torch.optim.AdamW(model_triplet.parameters(), lr=1e-4, weight_decay=0.025)

# Define learning rate scheduler with CosineAnnealingLR
# lr starts at the initial lr for the optimizer and gradually decreases by following a cosine curve.
# Helps to escape local minima and leads to better overal performance.
# overfitting --> use ReduceLROnPlateau
#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3) #patience=4

# Define loss function
criterion = nn.TripletMarginLoss()

# # diff: with upsampling waldo and downsampling notwaldo


# # Create an instance of ViT model
# image_size = 128
# patch_size = 8
# unique_classes = np.unique(train_loader.dataset.labels)
# num_classes = len(unique_classes)
# # overfitting --> change dim = 512 to 256
# #dim = 512
# # got worse so stick to dim = 512
# #dim = 256
# # this is the best try of dim --> increase weight_decay=0.01 to 0.02
# dim = 512

# # overfitting --> change num_layers = 3 to 2
# #num_layers = 3
# # got worse so stick to num_layers=3
# #num_layers = 2
# # this is the best try of num_layers --> decrease dim=512 to 256
# num_layers = 3

# heads = 8
# # overfitting --> increase dropout = 0.1 to 0.2
# #dropout = 0.1
# # better but still overfitting --> increase dropout = 0.2 to 0.3 (if doesn't work stick to this)
# #dropout = 0.2
# # got worse --> stick to dropout=0.2
# #dropout = 0.3
# # this is the best try of dropout --> decrease num_layers=3 to 2
# dropout = 0.125 # previousely 0.2

# model_triplet = ViT(image_size = image_size,
#             patch_size = patch_size,
#             num_classes = num_classes,
#             dim = dim,
#             num_layers = num_layers,
#             heads = heads,
#             dropout = dropout).to(device)

# # Define optimizer
# ## Not learning: loss does not decrease --> decrease lr=0.01 to lr=1e-3
# #optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# ## Not learning: loss does not decrease --> change optimizer to AdamW
# #optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)
# # AdamW is most widely used optimizer for Transformers according to https://arxiv.org/pdf/2302.01107.pdf
# # Overfitting! train loss decreases but val loss is not that much good, neg dist ar good, pos dist somehow... --> change scheduler from CosineAnnealingLR to ReduceLROnPlateau
# # better but still overfitting --> decrease lr from 5e-4 to lr=1e-4 & use ReduceLROnPlateau
# #optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)
# #################################### much more better (congrats!) but still not generalizes well --> decrease lr from 1e-4 to lr=5e-5 (if didn't work keep this)
# #optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
# # got worse, so stick to the lr=1e-4 and focus on tuning other hyperparameters --> increase dropout from 0.1 to dropout=0.2
# #optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)
# # this is the best try of adjusting lr --> increase dropout=0.1 to 0.2
# # adjust weight_decay=0.01 to 0.02
# #optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
# # got worse so stick to weight_decay=0.01
# #optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.02)
# # this is the best try of weight_decay --> ?
# optimizer = torch.optim.AdamW(model_triplet.parameters(), lr=1e-4, weight_decay=0.02) # previousely weight_decay=0.01

# # Define learning rate scheduler with CosineAnnealingLR
# # lr starts at the initial lr for the optimizer and gradually decreases by following a cosine curve.
# # Helps to escape local minima and leads to better overal performance.
# # overfitting --> use ReduceLROnPlateau
# #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, cooldown=2)

# # Define loss function
# criterion = nn.TripletMarginLoss()

print(model_triplet)

"""## 2.4 Train and Test Function

In this step we tried to train the model with and without hard negative mining. Although, the model will benefit from hard negative mining, it takes too much time to train each epoch (about 30 minutes each),therefore, we proceed without hard negative mining.
"""

#diff: without considering hard negatives


# Define training loop funtion for one iteration
print_every = 50
def train(epoch):
    model_triplet.train()

    train_loss = 0
    train_positive_dist = 0
    train_negative_dist = 0
    total = 0

    for batch_idx, batch in enumerate(train_loader):
        anchor, positive, negative, labels = batch

        anchor = anchor.to(device)
        positive = positive.to(device)
        negative = negative.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        anchor_out = model_triplet(anchor)
        positive_out = model_triplet(positive)
        negative_out = model_triplet(negative)

        loss = criterion(anchor_out, positive_out, negative_out)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

        # Calculate L2 norm
        # https://stackoverflow.com/questions/68489765/what-is-the-correct-way-to-calculate-the-norm-1-norm-and-2-norm-of-vectors-in
        # https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html#torch.linalg.vector_norm
        positive_dist = torch.linalg.vector_norm(anchor_out - positive_out, ord=2, dim=1)
        negative_dist = torch.linalg.vector_norm(anchor_out - negative_out, ord=2, dim=1)

        train_positive_dist += positive_dist.sum().item()
        train_negative_dist += negative_dist.sum().item()

        total += labels.size(0)

        epoch_loss = train_loss/(batch_idx+1)
        epoch_positive_dist = train_positive_dist / total
        epoch_negative_dist = train_negative_dist / total

        if batch_idx % print_every == 0:
            print('Epoch {}/{}, Iter {}/{}, Train Loss: {:.3f}, Train Positive Distance: {:.3f}, , Train Negative Distance: {:.3f}'.format(epoch, num_epochs, batch_idx, len(train_loader),
                                                                                                                                          epoch_loss, epoch_positive_dist, epoch_negative_dist))

    return epoch_loss, epoch_positive_dist, epoch_negative_dist

# # diff: with hard negative mining

# # Define training loop funtion for one iteration
# # WPO7
# print_every = 61
# def train(epoch):
#     model_triplet.train()

#     train_loss = 0
#     train_positive_dist = 0
#     train_negative_dist = 0
#     total = 0

#     for batch_idx, batch in enumerate(train_loader):
#         anchor, positive, negative, labels = batch

#         anchor = anchor.to(device)
#         positive = positive.to(device)
#         labels = labels.to(device)

#         optimizer.zero_grad()

#         anchor_out = model_triplet(anchor)
#         positive_out = model_triplet(positive)

#         # Hard Negative Mining for each anchor in the batch
#         hard_negative_outs = []
#         # Loop through each anchor in the batch
#         for i in range(anchor.size(0)):
#             # Set min distance to infinity
#             min_dist = float('inf')
#             hard_negative_out = None
#             # for each negative sample for that specific anchor in the batch
#             for neg in negative:
#                 neg = neg.to(device)
#                 neg_out = model_triplet(neg.unsqueeze(0))
#                 # Calculate L2 norm for anchoe[i] and each negative image in a batch
#                 dist = torch.linalg.vector_norm(anchor_out[i] - neg_out, ord=2, dim=1)
#                 if dist < min_dist:
#                     min_dist = dist
#                     hard_negative_out = neg_out
#             hard_negative_outs.append(hard_negative_out.squeeze(0))

#         hard_negative_outs = torch.stack(hard_negative_outs)

#         loss = criterion(anchor_out, positive_out, hard_negative_out)
#         loss.backward()
#         optimizer.step()

#         train_loss += loss.item()

#         # Calculate L2 norm
#         # https://stackoverflow.com/questions/68489765/what-is-the-correct-way-to-calculate-the-norm-1-norm-and-2-norm-of-vectors-in
#         # https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html#torch.linalg.vector_norm
#         positive_dist = torch.linalg.vector_norm(anchor_out - positive_out, ord=2, dim=1)
#         negative_dist = torch.linalg.vector_norm(anchor_out - hard_negative_out, ord=2, dim=1)

#         train_positive_dist += positive_dist.sum().item()
#         train_negative_dist += negative_dist.sum().item()

#         total += labels.size(0)

#         epoch_loss = train_loss/(batch_idx+1)
#         epoch_positive_dist = train_positive_dist / total
#         epoch_negative_dist = train_negative_dist / total

#         if batch_idx % print_every == 0:
#             print('Epoch {}/{}, Iter {}/{}, Train Loss: {:.3f}, Train Positive Distance: {:.3f}, , Train Negative Distance: {:.3f}'.format(epoch, num_epochs, batch_idx, len(train_loader),
#                                                                                                                                           epoch_loss, epoch_positive_dist, epoch_negative_dist))

#     return epoch_loss, epoch_positive_dist, epoch_negative_dist

# Define test loop funtion for one iteration for validation dataloader
# WPO7
def test(testloader):
    model_triplet.eval()

    test_loss = 0
    test_positive_dist = 0
    test_negative_dist = 0
    total = 0

    with torch.no_grad():
        for batch_idx, batch in enumerate(testloader):
            anchor, positive, negative, labels = batch

            anchor = anchor.to(device)
            positive = positive.to(device)
            negative = negative.to(device)
            labels = labels.to(device)

            anchor_out = model_triplet(anchor)
            positive_out = model_triplet(positive)
            negative_out = model_triplet(negative)

            loss = criterion(anchor_out, positive_out, negative_out)

            test_loss += loss.item()

            # Calculate L2 norm
            # https://stackoverflow.com/questions/68489765/what-is-the-correct-way-to-calculate-the-norm-1-norm-and-2-norm-of-vectors-in
            # https://pytorch.org/docs/stable/generated/torch.linalg.vector_norm.html#torch.linalg.vector_norm
            positive_dist = torch.linalg.vector_norm(anchor_out - positive_out, ord=2, dim=1)
            negative_dist = torch.linalg.vector_norm(anchor_out - negative_out, ord=2, dim=1)

            test_positive_dist += positive_dist.sum().item()
            test_negative_dist += negative_dist.sum().item()

            total += labels.size(0)

    loss = test_loss/(batch_idx+1)
    positive_dist = test_positive_dist / total
    negative_dist = test_negative_dist / total
    print('Test Loss: {:.3f}, Test Positive Distance: {:.3f}, , Test Negative Distance: {:.3f}'.format(loss, positive_dist, negative_dist))
    return loss, positive_dist, negative_dist

"""## 2.5 Evaluation of Model on validation set"""

# Run model
# WPO7

best_epoch = -1
best_loss = float('inf')
best_positive_dist = float('inf')
best_negative_dist = float('-inf')
best_dist = float('-inf')

train_losses = []
train_positive_dists = []
train_negative_dists = []

test_losses = []
test_positive_dists = []
test_negative_dists = []
test_dists = []

for epoch in range(num_epochs):
    train_epoch_loss, train_epoch_positive_dist, train_epoch_negative_dist = train(epoch)
    train_losses.append(train_epoch_loss)
    train_positive_dists.append(train_epoch_positive_dist)
    train_negative_dists.append(train_epoch_negative_dist)

    torch.cuda.empty_cache()

    test_loss, test_positive_dist, test_negative_dist = test(val_loader)
    test_losses.append(test_loss)
    test_positive_dists.append(test_positive_dist)
    test_negative_dists.append(test_negative_dist)
    # Calculate the distance between positive and negative pairs
    test_dist = test_negative_dist - test_positive_dist
    test_dists.append(test_dist)


    scheduler.step(test_loss)

    torch.cuda.empty_cache()

    # Create a dictionary to store model's parameter (state_dict), loss, positive_dist, negative_dist and the epoch number
    state = {'model': model_triplet.state_dict(),
             'loss': test_loss,
             'positive_dist': test_positive_dist,
             'negative_dist': test_negative_dist,
             'epoch': epoch}

    # If the test_dist is bigger than the so far best_dist, state will be saved in a file named model.pth
    if test_dist > best_dist:
       # If you want to save it on your google drive
       save_path = os.path.join(model_state_path, 'model_triplet.pth')
       torch.save(state, save_path)
       # If you want to save it in the current working directory (usually "/content")
       torch.save(state, 'model_triplet.pth')
       best_epoch = epoch
       best_loss = test_loss
       best_positive_dist = test_positive_dist
       best_negative_dist = test_negative_dist
print("\n" + "===================================================================" + "\n")
print(f"Best model is at epoch {best_epoch}")
print(f"Loss: {best_loss}")
print(f"Positive distance: {best_positive_dist}")
print(f"Negative distance: {best_negative_dist}")

# Plot evaluation results
fig, axs = plt.subplots(1, 3, figsize=(15, 5))  # 1 row, 3 columns

# Adjust space between the subplots
fig.subplots_adjust(wspace=0.3)

# Plot loss
axs[0].plot(train_losses, label='Train')
axs[0].plot(test_losses, label='Validation')
axs[0].set_xlabel('Epoch')
axs[0].set_ylabel('Loss')
axs[0].set_title('Loss Per Epoch')
axs[0].legend()

# Plot positive distances
axs[1].plot(train_positive_dists, label='Train')
axs[1].plot(test_positive_dists, label='Validation')
axs[1].set_xlabel('Epoch')
axs[1].set_ylabel('Positive Distance')
axs[1].set_title('Positive Distance Per Epoch')
axs[1].legend()

# Plot negative distances
axs[2].plot(train_negative_dists, label='Train')
axs[2].plot(test_negative_dists, label='Validation')
axs[2].set_xlabel('Epoch')
axs[2].set_ylabel('Negative Distance')
axs[2].set_title('Negative Distance Per Epoch')
axs[2].legend()

plt.show()

"""## 2.6 Evaluation of Model on test set"""

model_triplet.eval()
test_loss, test_positive_dist, test_negative_dist = test(test_loader)

"""# **Part 3: Re-train the model for binary classification**

## Initialize Hyperparameters
"""

batch_size = 200
num_epochs = 20
num_workers = 1

"""## **3.1 Data Preparation**

### **3.1.1 Split the dataset to train/validation/test sets**
"""

# # Load images and labels and preprocess them
# all_images, all_labels = preprocess_images(data_dir)
# print(len(all_images))
# # Split data: 90% for training and 10% for testing & 10% of training for validation
# # random_state ensures reproducability, shuffle ensures randomization
# # stratify ensures being representative of the entire dataset (having the same proportion of different classes in splitting)
# train_images, test_images, train_labels, test_labels = train_test_split(all_images, all_labels, test_size=0.1, random_state=42, shuffle=True, stratify=all_labels)
# train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.1, random_state=42, shuffle=True, stratify=train_labels)

# print(f"Training size: {len(train_images)}")
# print(f"Training size Waldo: {np.sum(train_labels == 1)}")
# print(f"Training size notWaldo: {np.sum(train_labels == 0)}")
# print(f"Validation size Waldo: {np.sum(val_labels == 1)}")
# print(f"Validation size notWaldo: {np.sum(val_labels == 0)}")
# print(f"Testing size Waldo: {np.sum(test_labels == 1)}")
# print(f"Testing size notWaldo: {np.sum(test_labels == 0)}")
# print(f"Validation size: {len(val_images)}")
# print(f"Test size: {len(test_images)}")

"""### **3.1.2 Training data augmentation**"""

# # diff: upsampling waldo with augmenting seperately + downsampling notwaldo

# # Define number of images for waldo and not waldo
# num_not_waldo = 3000
# num_waldo = 3000 # in total will be +53 (already existant waldo samples)
# ## Using more than 3000 is worse
# num_duplicate_waldo = math.floor(num_waldo / np.sum(train_labels == 1))

# # Separate images with and without Waldo in the training set
# waldo_images = [img for img, label in zip(train_images, train_labels) if label == 1]
# not_waldo_images = [img for img, label in zip(train_images, train_labels) if label == 0]
# # Sampling not-Waldo images
# sampled_not_waldo_images = random.sample(not_waldo_images, num_not_waldo)

# # Convert numpy arrays to PIL Images for applying transformation
# waldo_images_pil = [Image.fromarray(img) for img in waldo_images]

# # Duplicate Waldo images and apply augmentation
# aug_waldo_images = []
# portion = num_duplicate_waldo // 5 # 5 is number of augmentation method

# for img in waldo_images_pil:
#     for i in range(num_duplicate_waldo):
#         if i < portion * 1:
#             transform = transforms.RandomHorizontalFlip(p=1.0)
#         elif i < portion * 2:
#             transform = transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=1.0)
#         elif i < portion * 3:
#             transform = transforms.RandomGrayscale(p=1.0)
#         elif i < portion * 4:
#             transform = transforms.RandomRotation(15)
#         else:
#             aug_transform = transforms.RandomVerticalFlip(p=1.0)

#         aug_img = transform(img)
#         aug_waldo_images.append(aug_img)


# # Combining the sampled non-Waldo images with all Waldo images
# final_train_images = waldo_images + sampled_not_waldo_images + aug_waldo_images
# final_train_labels = [1] * len(waldo_images) + [0] * len(sampled_not_waldo_images) + [1] * len(aug_waldo_images)

# # Combine and shuffle the final training set
# combined_set = list(zip(final_train_images, final_train_labels))
# random.shuffle(combined_set)
# final_train_images, final_train_labels = zip(*combined_set)


# # Convert PIL Image to a numPy array (because of warnings)
# train_images_np = [np.array(img) for img in final_train_images]

# # Convert the list of numPy arrays to a single NumPy array (because of warnings)
# train_images = np.array(train_images_np)
# train_labels = np.array(final_train_labels)

# # Print final dataset sizes
# print(f"Training size: {len(train_images)}")
# print(f"Training size Waldo: {np.sum(train_labels == 1)}")
# print(f"Training size notWaldo: {np.sum(train_labels == 0)}")
# print(f"Validation size: {len(val_images)}")
# print(f"Test size: {len(test_images)}")

# def calculate_mean_std(loader):
#     mean, std, num_batches = 0, 0, 0

#     for img, _, in loader: ## Different than the other function because the dataloader returns less variables
#         batch_size = img.size(0)
#         # Reshape the image from [B, C, W, H] to [B, C, W * H] Batch, Channel, Width, Height
#         # keep the first and second element at their position but flatten the rest
#         # W*H is the total number of pixels in each channel of an image
#         img = img.view(img.size(0), img.size(1), -1)
#         # Get the mean of each image and sum them over batch
#         mean += img.mean(2).sum(0)
#         # Get the std of each image and sum them over batch
#         std += img.std(2).sum(0)
#         # Add the number of images in one batch (batch_size) to number of batches
#         num_batches += batch_size

#     mean /= num_batches
#     std /= num_batches
#     return mean, std

class Binary(Dataset):
    def __init__(self, img, lbl, trans=None):
        self.images = img
        self.labels = lbl
        self.transform = trans
        self.to_pil = transforms.ToPILImage()

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, index):
        if self.transform:
          image = self.transform( self.to_pil( self.images[index]  ))
        else:
          self.images[index]
        label = torch.Tensor([self.labels[index]])  # Convert label to tensor
        return image, label

# # Create a temporary dataset to calculate mean and std of training images
# temp_dataset = Binary(train_images, train_labels, trans=transforms.ToTensor())
# temp_loader = DataLoader(temp_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

# # Calculate mean and std
# # calculation of mean and std is only based on training data but it is applied on all datasets
# mean_train, std_train = calculate_mean_std(temp_loader)

# print(f"Mean of training set: {mean_train}")
# print(f"STD of training set: {std_train}")

"""### **3.1.5. Create DataLoaders for training/validation/test**"""

##Transformations for data augmentation
transform_train = transforms.Compose([transforms.RandomRotation(10),
                                      transforms.RandomHorizontalFlip(),
                                      transforms.ToTensor(),
                                      transforms.Normalize(mean, std)
                                      ])

transform_test_val = transforms.Compose([transforms.ToTensor(),
                                         transforms.Normalize(mean, std)
                                        ])


# Create datasets
train_set_bi = Binary(train_images, train_labels, trans=transform_train)
val_set_bi = Binary(val_images, val_labels, trans=transform_test_val)
test_set_bi = Binary(test_images, test_labels, trans=transform_test_val)

# Create DataLoaders
train_loader_bi = DataLoader(train_set_bi, batch_size=batch_size, shuffle=True, num_workers=num_workers)
val_loader_bi = DataLoader(val_set_bi, batch_size=batch_size, shuffle=False, num_workers=num_workers)
test_loader_bi = DataLoader(test_set_bi, batch_size=batch_size, shuffle=False, num_workers=num_workers)

"""## **3.2 Transfer Learning**

Transfer learning was used to take the model after the first training and re-train it for binary classification.
Model's parameters were saved in a file model_triplet.pth. So I had to take that model and apply the trained weights and bias.
Tried to import the ViT class from the original DL_Project and add different layers to de neuronal network to do the binary classification but there were some problems, f. i. !git clone https://github.com/vc1492a/Hey-Waldo.git line of the original file gives problems.

Instead, I created a new file called  model_ViT.ipynb with only the
definition of the ViT, Patchify, FeedForward, MultiHeadAttention and
Transformer. Then I saved it as model_ViT.py So I could import the model's base and apply the weights and bias stored at the .pyh.

I used the following code:
"""

# Add the DL_Project folder to the sys path
# dl_project_path = os.path.join('/content/drive/MyDrive/')
# sys.path.append(dl_project_path)
# from vit_class import ViT

# image_size = 128
# patch_size = 8
# num_classes = 2
# heads = 8
# dim = 512
# num_layers = 3
# dropout = x

# model = ViT(image_size = image_size,
#             patch_size = patch_size,
#             num_classes = num_classes,
#             dim = dim,
#             num_layers = num_layers,
#             heads = heads,
#             dropout = dropout).to(device)

# model_path = '/content/drive/MyDrive/model_triplet.pth'
# Load the model parameters
# checkpoint = torch.load(model_path, map_location=torch.device('cpu'))
# model_state_dict = model.load_state_dict(checkpoint['model'])

"""Afterwards, I needed to add some layers in the end of the Neuronal Network. It seemed to work because the code did not give errors and you could see the implemented layers when printing the model. However, when making predictions, the output data had the shape of the last layer of the initial model, not the modified one.

So, I made another last change to make it work. I modified the model_ViT.ipynb adding the layers I wanted in the end, so I had what was required after importing the class.

Because of this, I had to change the code when importing the trained weights and biases. I needed to add "strict=False" to:

    model_state_dict = model.load_state_dict(checkpoint['model'], strict=False)

 Thus, although the model parameters have different size than the new model, parameters fit good and the new layers are initialised with random numbers.

This is a binary classification task, so 1 output feature is needed in the model. A sigmoid activation function is required as well in order to have a probability between 0 and 1 in the output. This probability will indicate how likely is to be Waldo. If the value is <0.5, the predicted class is NotWaldo. Otherwise, if the value>0.5, the prediction is Waldo.

On the other hand, multiple hidden layers can be used to make a deeper neuronal network, as well as intermediate activation functions and dropouts. Multiple choices have been tried and the best best performance so far is one layer from 512 input features to 1 output features and a sigmoid activation layer. More layers can produce a greater overfitting to the training images.

In this code, the required class is going to be defined as follows.
"""

# Define ViT model by combining all the modules we defined above, into our final ViT model.
# WPO7
class ViT_binary(nn.Module):
    def __init__(self, image_size, patch_size, num_classes, dim, num_layers, heads, dropout):
        '''
        dim: Dimension of patch embeddings.
        num_layers: Number of transformer layers.
        heads: Number of attention heads in the multi-head attention mechanism.
        '''
        super().__init__()

        self.patchify = Patchify(image_size, patch_size, dim)
        # positional encoding provides information about the position of the patches in the input sequence.
        # helps the model to consider the position of each patch when processing the image.
        # nn.Parameter means it's a learnable parameter and is updated during training.
        # (1, self.patchify.num_patches, dim) is the shape of position for each patch.
        self.pos_encoding = nn.Parameter(torch.randn(1, self.patchify.num_patches, dim))
        self.transformer = Transformer(dim, num_layers, heads, dropout = dropout)

        ## Binary classification layer 1
        self.layer_class1 = nn.Linear(dim, 1)
        self.sigmoid = nn.Sigmoid()


        ## Binary classification layer 2
        # self.layer_class2 = nn.Linear(dim//2, 1)
        #self.dropout_c = nn.Dropout(dropout)
        # self.relu = nn.ReLU()

    def forward(self, x):
        x = self.patchify(x)         # (batch_size, N, dim)
        x = x + self.pos_encoding    # (batch_size, N, dim)
        x = self.transformer(x)      # (batch_size, N, dim)
        x = x.mean(1)                # (batch_size, dim)


        x = self.layer_class1(x)       # (batch_size, N, 1)
        x = self.sigmoid(x)


        # Tried with the following code to add 2 (tried with even 3 layers) but the performance is similar. It overfits faster.
        # x = self.relu(self.layer_class1(x))       # (batch_size, N, dim/2)
        # x = self.dropout_c(x)                     # (batch_size, N, dim/2)
        # x = self.layer_class2(x)                  # (batch_size, N, 1)
        # x = self.sigmoid(x)                       # (batch_size, N, 1)

        return x                                    # (batch_size, N, 1)

"""Using 3 or 2 layers, the model overfits easily. A simpler model is better regarding the overfitting, but the perform is similar."""

image_size = 128    ## Size of the images
patch_size = 8
num_classes = 2
heads = 8
dim = 512
num_layers = 3  ## RGB images
dropout = 0.3 ## Increase dropout is typically used to control overfitting. However, the model overfits regardless of the dropout value.

model = ViT_binary(image_size = image_size,
            patch_size = patch_size,
            num_classes = num_classes,
            dim = dim,
            num_layers = num_layers,
            heads = heads,
            dropout = dropout).to(device)

# Load the model parameters
## The next commented line is used to import the model document model_triplet.pth with the weight and biases of the pretrained model
parameters_pretrained = torch.load('/content/drive/MyDrive/model_triplet.pth')
model.load_state_dict(parameters_pretrained['model'], strict=False)

"""Print the model to see the architecture"""

model

"""There are two different ways to train the model:


*   Train all the different model layers with a small learning rate to do small steps of the weights and bias.
*   Freeze the layers of the pre-trained model and train only the layers on the bottom. Higher values of the lr can be used.

Both approaches were tried and the second one provides better results, in the first one the model doesn't learn This can happen because the pre-trained model do not have enough accuracy for the task and the parameters of the new layer barely change.


"""

# to activate all the parameters in the model
# for p in model.parameters():
#     p.requires_grad = True

# Only activate the layers in the top
for n, p in model.named_parameters():
    if n.startswith('layer_class1'):
        p.requires_grad = True  ## When optimizer_bi.step() the weights and bieases will change
    else:
        p.requires_grad = False

"""## **3.3 Train the model by binary classification**

### Train function
"""

import time
import copy
threshold = 0.5         ## used for binary classification threshold. A higher probability than that will be Waldo
def train_model_bi(train_loader_bi):
    model.train()
    train_loss = 0
    correct = 0

    train_acc_sum = 0
    all_preds = []
    all_labels = []

    for b_index, (im, l) in enumerate(train_loader_bi):
      im = im.to(device)
      l = l.to(device)
      # forward pass
      outputs = model(im)                       ## Make a prediction with the batch images
      #preds = (outputs > 0.5).float()
      l = l.reshape(-1)
      preds = outputs.reshape(-1)
      #labels = labels.float()
      #labels.requires_grad_()
      preds.requires_grad_()                    ## there are error if I do not activate the gradient for predictions
      #preds = preds.float()

      loss = loss_fn(preds, l)                  ## Compute the loss
      optimizer_bi.zero_grad()                  ## When the forward pass, pytorch computes gradients in the parameters, they are set to zero for the backward pass
      loss.backward()                           ## Go backward in the neuronal network and compute the weights and biases with the computed loss
      optimizer_bi.step()                       ## Updating the model parameters with the optimizer

      ### Save the loss and the accuracy of training
      train_loss += loss.item()
      train_acc = 100*(preds.round() == l).float().mean()   ## If the value of the prediction is between 0 and 0.5, NotWaldo predicted
                                                            ## if prediction > 0.5. Waldo predicted.
      train_acc_sum += train_acc

      ## Save the predictions and labels of the training
      preds_binary = (outputs > threshold).float()
      preds_binary = preds_binary.reshape(-1)
      all_preds.extend(preds_binary.cpu().numpy())
      all_labels.extend(l.cpu().numpy())

    train_epoch_acc =  train_acc_sum / (b_index+1)
    train_epoch_loss = train_loss/(b_index+1)
    return all_preds, all_labels, train_epoch_loss, train_epoch_acc

"""### Test function"""

# Define validatoin loop funtion for one iteration for validation dataloader
def test_model_bi(test_loader):
    model.eval()
    val_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for b_index, (im, l) in enumerate(test_loader):
            im = im.to(device)
            l = l.to(device)

            outputs = model(im)
            preds = outputs
            l = l.reshape(-1)
            preds = preds.reshape(-1)

            #labels = labels.float()
            #labels.requires_grad_()
            # preds.requires_grad_()
            #preds = preds.float()
            loss = loss_fn(preds, l)
            preds_binary = (outputs > threshold).float()    ## If the value of the prediction is between 0 and 0.5, NotWaldo predicted
                                                      ## if prediction > 0.5. Waldo predicted.
            preds_binary = preds_binary.reshape(-1)
            val_loss += loss.item()

            # Collect predictions and true labels for computing metrics at the end
            all_preds.extend(preds_binary.cpu().numpy())
            all_labels.extend(l.cpu().numpy())


    loss = val_loss/(b_index+1)

    return all_labels, all_preds, loss

"""### Precision, recall, f1 and confusion matrix functions

### Set Parameters for training: criterion, optimizer and scheduler

*   Loss_fn: The Binary Cross-Entropy (BCE) is widely used for classification models.
*   Optimizer: only changes the parameters of the fully connected layer on the top of the model. The lr is 0.0001, to learn little by little. If not, the training accuracy increases really fast. However, the validation F1 increases slowly and do not reach a good value.
*   Scheduler: used to decrement the learning rate in each poch to perform the learning. As the model approaches the best fitting weights and biases, learning must be slowed down to fit more accurately.
"""

## https://neptune.ai/blog/pytorch-loss-functions
loss_fn =  nn.BCELoss()
#criterion_bi = nn.BCEWithLogitsLoss()

optimizer_bi = torch.optim.SGD(model.layer_class1.parameters(), lr=0.0001, weight_decay=0.3)
#optimizer_bi = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)

## Scheduler useful to tone the lr. In the begining the lr is high and it gets reduced.
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_bi, T_max=num_epochs)
#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer_bi, gamma=0.1)
#scheduler = torch.optim.lr_scheduler.StepLR(optimizer_bi, step_size=5, gamma=0.1)

"""### Training"""

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns

## Create a result dictionary in case in case it is not created later. Avoid errors
results = {
          "epoch": 0,
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "loss_train": 0,
          "loss_test": 0,
          "acc_train": 0,
          "acc_test": 0,
          "labels": 0,
          "preds":0,
          "model": 0
          }


best_acc = 0

## Store the losses, accuracies and (precision, recall and f1) for graphing
losses_train = []
losses_test = []
accuracies_train = []

precision = []
recall = []
f1 = []

best_f1 = 0                    ## to store the best epoch results, wich has the best f1 score


print("Starting Training...")
for epoch in range(num_epochs):
    preds_train, labels_train, train_loss, train_acc = train_model_bi(train_loader_bi)
    losses_train.append(train_loss)
    accuracies_train.append(train_acc.cpu().numpy())

    torch.cuda.empty_cache()

    scheduler.step()  ## Reduce the learning rate
    labels_val, preds_val, loss = test_model_bi(val_loader_bi) ## Test the performance of the model in the validation dataset

    # Compute precision, recall, and F1 score
    labels_tensor = torch.tensor(labels_val)
    preds_tensor = torch.tensor(preds_val)
    p = precision_score(labels_tensor, preds_tensor)
    r = recall_score(labels_tensor, preds_tensor)
    f = f1_score(labels_tensor, preds_tensor)


    ## Print all the indicators
    print('Epoch {} / {}'.format(epoch, num_epochs))
    print('Train Loss: {:.3f}'.format(train_loss))
    print('Val Loss: {:.3f}'.format(loss))
    print('Train Accuracy: {:.3f}'.format(train_acc))
    print('Precision: {:.3f}, Recall: {:.3f}, F1-score: {:.3f}'.format(p, r, f))

    ## Make a list to graph
    precision.append(p)
    recall.append(r)
    f1.append(f)

    ## Store the best f1-score performance of the epoches. This is going to be stored and used in the test set.
    if f>best_f1:
      best_f1 = f
      results = {
          "epoch": epoch+1,
          "precision": p,
          "recall": r,
          "f1": f,
          "loss_train": train_loss,
          "loss_test": loss,
          "acc_train": train_acc,
          "labels_train": labels_train,
          "preds_train":preds_train,
          "labels_val": labels_val,
          "preds_val":preds_val,
          "model": model.state_dict()
          }

    losses_test.append(loss)
    torch.cuda.empty_cache()

model_to_save = {
    "precision": results["precision"],
    "recall": results["recall"],
    "f1": results["f1"],
    "loss_train": results["loss_train"],
    "loss_test": results["loss_test"],
    "acc_train": results["acc_train"],
    "model": results["model"]
    }

# Define path to save model's state file
path = os.path.join(model_state_path, 'model_classification.pth')
torch.save(model_to_save, path)

"""## **3.4 Results**

### Print the results for the best F1-score of the epochs

#### Validation dataset best performance
"""

print("Best results in validation set:")
for key, value in results.items():
  if key!="model" and  key!="labels_train" and key!="preds_train" and key!="labels_val" and key!="preds_val":
    print("{}: {:.3f}".format(key, value))

matrix = confusion_matrix(results["labels_val"], results["preds_val"])
sns.heatmap(matrix, xticklabels=['notwaldo', 'waldo'], yticklabels=['notwaldo', 'waldo'], annot=True, fmt="d")

"""#### Training dataset"""

p_train = precision_score(results["labels_train"], results["preds_train"])
r_train = recall_score(results["labels_train"], results["preds_train"])
f_train = f1_score(results["labels_train"], results["preds_train"])
print("In training dataset:")
print("Precision: {:.3}, Recall: {:.3}, F1: {:.3}".format(p_train, r_train, f_train))
print("Accuracy in training: {:.3}%".format(results["acc_train"]))
matrix = confusion_matrix(results["labels_train"], results["preds_train"])
sns.heatmap(matrix, xticklabels=['notwaldo', 'waldo'], yticklabels=['notwaldo', 'waldo'], annot=True, fmt="d")

"""The model overfits from the beginning, so the indicators are high. The performance in the training set is awesome but the model does not know to predict new data in the validation and test datasets.

### Precision, recall and f1 evolution in validation dataset
"""

plt.plot(precision, label="precision")
plt.plot(recall, label="recall")
plt.plot(f1, label="f1")

plt.xlabel('epoch')
plt.ylabel('Value')
plt.legend()

"""Precision, recall, and the most important indicator, f1, go to low values of 0.333, and stagnate.

### Losses evolution
"""

plt.plot(losses_test, label='val')
plt.plot(losses_train, label='train')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()

"""The final train loss is 0.451 and the final val loss is 0.363

### Accuracies evolution
"""

plt.plot(accuracies_train)
plt.xlabel('epoch')
plt.ylabel('Train accuracy')
plt.legend()

"""The train accuracy goes slowly to 97% because the learning rate is 0.0001. If f.i., lr was 0.1, it would be 90% in the second epoch."""

# Crear el primer conjunto de ejes
_, accuracy_plot = plt.subplots()

# Plot train accuracy
accuracy_plot.plot(accuracies_train, 'g-')
accuracy_plot.set_xlabel('Batch')
accuracy_plot.set_ylabel('Accuracies', color='g')

f1_plot = accuracy_plot.twinx()

# Plot F1
f1_plot.plot(f1, 'r-')
f1_plot.set_ylabel('f1', color='r')
plt.show()

"""The F1 increases as the training accuracy increases, but the f1 only reach goes to 0.364 and the objective is to be as closes as possible to 1."""

model.load_state_dict(results['model'])

labels, preds, loss = test_model_bi(test_loader_bi)

# Compute precision, recall, and F1 score
labels_tensor = torch.tensor(labels)
preds_tensor = torch.tensor(preds)
p2 = precision_score(labels_tensor, preds_tensor)
r2 = recall_score(labels_tensor, preds_tensor)
f2 = f1_score(labels_tensor, preds_tensor)
print("In testing dataset:")
print("Precision: {}".format(p2))
print("Recall: {}".format(r2))
print("F1-score: {}".format(f2))
conf_matrix = confusion_matrix(labels, preds)
sns.heatmap(conf_matrix, xticklabels=['notwaldo', 'waldo'], yticklabels=['notwaldo', 'waldo'], annot=True, fmt="d");

"""In the images that the model did not see before (using the test dataset), the results are not good. No Waldo images are predicted as Waldo. There are only 7 Waldo images, would be great to the results with more.

# **Part 4: Train the model on detecting instances**
"""

# # Split the dataset into training and testing sets
# train_ratio = 0.9
# num_train = int(len(image_paths) * train_ratio)

# train_images = image_paths[:num_train]
# test_images = image_paths[num_train:]
# train_annotations = annotations[:num_train]
# test_annotations = annotations[num_train:]

# # Function to crop 9 distinct regions from an image
# def crop_regions(image, annotations):
#     cropped_images = []
#     for i in range(3):
#         for j in range(3):
#             x_start = i * (image.shape[1] // 3)
#             y_start = j * (image.shape[0] // 3)
#             x_end = (i + 1) * (image.shape[1] // 3)
#             y_end = (j + 1) * (image.shape[0] // 3)

#             cropped_img = image[y_start:y_end, x_start:x_end]
#             cropped_annotations = []

#             for annotation in annotations:
#                 x, y, w, h = annotation
#                 # Check if the bounding box overlaps with the cropped region
#                 if x_start < x + w and x < x_end and y_start < y + h and y < y_end:
#                     # Adjust the bounding box coordinates relative to the cropped region
#                     cropped_annotations.append((x - x_start, y - y_start, w, h))

#             cropped_images.append((cropped_img, cropped_annotations))

#     return cropped_images

# # Iterate through each training image and create 9 cropped regions
# for image_path, annotation in zip(train_images, train_annotations):
#     # Load the image
#     image = cv2.imread(image_path)

#     # Crop 9 distinct regions
#     cropped_regions = crop_regions(image, [annotation])

#     # Save the cropped images and corresponding annotations
#     for idx, (cropped_img, cropped_annotation) in enumerate(cropped_regions):
#         save_path = os.path.join("train_cropped", f"image_{idx}_{os.path.basename(image_path)}")
#         cv2.imwrite(save_path, cropped_img)
#         # Save the annotations as well
#         np.save(save_path.replace(".jpg", "_annotations.npy"), np.array(cropped_annotation))

def resize_images(input_folder, output_folder, new_size):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for filename in os.listdir(input_folder):
        if filename.endswith(('.jpg', '.jpeg', '.png')):
            try:
                img_path = os.path.join(input_folder, filename)
                img = Image.open(img_path)
                img_resized = img.resize(new_size, Image.ANTIALIAS)  # Resize the image
                output_path = os.path.join(output_folder, filename)
                img_resized.save(output_path)
            except Exception as e:
                print(f"Error processing {filename}: {e}")

# Provide input and output directories along with the desired new size
input_directory = "/content/waldo/train/img"
output_directory = "/content/waldo/train/img"
new_size = (224, 224)  # Change this to your desired dimensions

resize_images(input_directory, output_directory, new_size)
# Provide input and output directories along with the desired new size
input_directory = "/content/waldo/val/val_img"
output_directory = "/content/waldo/val/val_img"
resize_images(input_directory, output_directory, new_size)

class ViTObjectDetection(nn.Module):
    def __init__(self, image_size, patch_size, num_classes, dim, num_layers, heads, dropout):
        super().__init__()

        self.patchify = Patchify(image_size, patch_size, dim)
        self.pos_encoding = nn.Parameter(torch.randn(1, self.patchify.num_patches, dim))
        self.transformer = Transformer(dim, num_layers, heads, dropout=dropout)

        # Binary classification layer 1
        self.layer_class1 = nn.Linear(dim, 1)
        self.sigmoid = nn.Sigmoid()

        # Bounding box prediction layer
        self.bbox_layer = nn.Linear(dim, 4)

    def forward(self, x):
        x = self.patchify(x)
        x = x + self.pos_encoding
        x = self.transformer(x)
        x = x.mean(1)

        # Binary classification
        x_class = self.layer_class1(x)
        waldo_prob = self.sigmoid(x_class)

        # Bounding box prediction
        bbox_output = self.bbox_layer(x)

        return bbox_output, waldo_prob

image_size = 128    ## Size of the images
patch_size = 8
num_classes = 2
heads = 8
dim = 512
num_layers = 3  ## RGB images
dropout = 0.3 ## Increase dropout is typically used to control overfitting. However, the model overfits regardless of the dropout value.

model = ViTObjectDetection(image_size = image_size,
            patch_size = patch_size,
            num_classes = num_classes,
            dim = dim,
            num_layers = num_layers,
            heads = heads,
            dropout = dropout).to(device)

# Load the model parameters
## The next commented line is used to import the model document model_triplet.pth with the weight and biases of the pretrained model
parameters_pretrained = torch.load('/content/drive/MyDrive/model_classification.pth')
model.load_state_dict(parameters_pretrained['model'], strict=False)

model

class CustomObjectDetectionDataset(Dataset):
    def __init__(self, data_dir, image_transform=None, bbox_transform=None, train=True):
        self.data_dir = data_dir
        self.image_transform = image_transform
        self.bbox_transform = bbox_transform
        self.train = train
        self.image_ids = self.load_image_ids()

    def load_image_ids(self):
        img_folder = os.path.join(self.data_dir, 'img')
        return [filename[:-4] for filename in os.listdir(img_folder) if filename.endswith('.jpg')]

    def load_annotation(self, image_id):
        label_folder = os.path.join(self.data_dir, 'labels')
        label_path = os.path.join(label_folder, f"{image_id}.txt")

        with open(label_path, 'r') as file:
            lines = file.readlines()
            # Extract the class label and the last four elements from the split result
            class_label = int(lines[0].split()[0])
            bbox = [float(coord) for coord in lines[-1].split()][-4:]

        return class_label, bbox

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        image_id = self.image_ids[idx]

        # Load image
        image_path = os.path.join(self.data_dir, 'img', f"{image_id}.jpg")
        image = Image.open(image_path).convert("RGB")

        # Load annotation (class label and bounding box)
        class_label, bbox = self.load_annotation(image_id)

        # Skip samples where the class label is 1
        if class_label == 1:
            return torch.zeros(3, 128, 128), 1, torch.zeros(4), image_id  # Placeholder tensors

        # Apply image transformations if provided
        if self.image_transform:
            image = self.image_transform(image)

        # Apply bounding box transformations if provided
        if self.bbox_transform:
            bbox = self.bbox_transform(bbox)

        # Convert bounding box to tensor
        bbox = torch.FloatTensor(bbox)

        return image, class_label, bbox, image_id
# Define transformations as needed (e.g., resizing, normalization, etc.)
image_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor()
])

# Bbox transform: Assuming you want to normalize bounding box coordinates
bbox_transform = transforms.Compose([transforms.Lambda(lambda x: [coord for coord in x])])

# Specify the path to your dataset
data_dir = '/content/drive/MyDrive/waldo/train'

# Create train dataset and loader
train_dataset = CustomObjectDetectionDataset(data_dir=data_dir,
                                             image_transform=image_transform,
                                             bbox_transform=bbox_transform,
                                             train=True)
train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)

# Create validation dataset and loader
val_dataset = CustomObjectDetectionDataset(data_dir='/content/drive/MyDrive/waldo/val',
                                           image_transform=image_transform,
                                           bbox_transform=bbox_transform,
                                           train=False)
val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)

# Print the length of train_loader and train_dataset
print(f"Length of train_loader: {len(train_loader)}")
print(f"Length of train_dataset: {len(train_dataset)}")

# import matplotlib.patches as patches

# def show_image_with_box_and_id(image, class_label, bbox, image_id, title='Image with Bounding Box, Class, and ID'):
#     # Convert tensor to NumPy array and transpose to (H, W, C)
#     img = image.numpy().transpose(1, 2, 0)

#     # Convert bounding box to original image coordinates
#     h, w, _ = img.shape
#     x_center, y_center, width, height = bbox
#     x_min = int((x_center - width / 2) * w)
#     y_min = int((y_center - height / 2) * h)
#     x_max = int((x_center + width / 2) * w)
#     y_max = int((y_center + height / 2) * h)

#     # Display the image
#     fig, ax = plt.subplots(figsize=(8, 8))
#     ax.imshow(img)
#     ax.set_title(f'Image ID: {image_id}, Class: {class_label}')

#     # Display the bounding box
#     rect = patches.Rectangle(
#         (x_min, y_min),
#         x_max - x_min,
#         y_max - y_min,
#         linewidth=2, edgecolor='r', facecolor='none'
#     )
#     ax.add_patch(rect)

#     plt.show()

# # Visualize a batch from the train_loader
# for batch_idx, (images, class_labels, bboxes, image_ids) in enumerate(train_loader):
#     # Iterate over individual samples in the batch
#     for i in range(len(images)):
#         # Print image ID, class label, and corresponding bounding box
#         print(f"Image ID: {image_ids[i]}, Class: {class_labels[i]}, Bounding Box: {bboxes[i]}")

#         # Display the image with bounding box and class
#         show_image_with_box_and_id(images[i], class_labels[i], bboxes[i], image_ids[i])

def calculate_iou(box1, box2):
    """
    Calculate IoU between two bounding boxes.

    Args:
        box1 (list or torch.Tensor): [x1, y1, x2, y2] coordinates of the first box.
        box2 (list or torch.Tensor): [x1, y1, x2, y2] coordinates of the second box.

    Returns:
        float: Intersection over Union (IoU) value.
    """
    x1, y1, x2, y2 = box1
    x3, y3, x4, y4 = box2

    # Calculate the intersection rectangle
    intersection_x1 = max(x1, x3)
    intersection_y1 = max(y1, y3)
    intersection_x2 = min(x2, x4)
    intersection_y2 = min(y2, y4)

    # Calculate area of intersection
    intersection_area = max(0, intersection_x2 - intersection_x1 + 1) * max(0, intersection_y2 - intersection_y1 + 1)

    # Calculate area of union
    area_box1 = (x2 - x1 + 1) * (y2 - y1 + 1)
    area_box2 = (x4 - x3 + 1) * (y4 - y3 + 1)
    union_area = area_box1 + area_box2 - intersection_area

    # Calculate IoU
    iou = intersection_area / union_area

    return iou

def calculate_map(ground_truths, predictions, iou_threshold=0.5):
    """
    Calculate mean Average Precision (mAP).

    Args:
        ground_truths (list): List of ground truth bounding boxes.
        predictions (list): List of predicted bounding boxes with associated confidence scores.
        iou_threshold (float): IoU threshold for considering a detection as correct.

    Returns:
        float: Mean Average Precision (mAP) value.
    """
    # Sort predictions by confidence score in descending order
    predictions.sort(key=lambda x: x['score'], reverse=True)

    true_positives = 0
    false_positives = 0
    precision = 0
    recall = 0

    for prediction in predictions:
        # Check if the prediction has a corresponding ground truth
        matching_gt_index = None
        for i, gt in enumerate(ground_truths):
            iou = calculate_iou(np.array(gt['bbox']), np.array(prediction['bbox']))
            if iou >= iou_threshold:
                matching_gt_index = i
                break

        if matching_gt_index is not None:
            true_positives += 1
            # Remove the matched ground truth to avoid double-counting
            del ground_truths[matching_gt_index]
        else:
            false_positives += 1

        # Calculate precision and recall at each step
        precision = true_positives / (true_positives + false_positives)
        recall = true_positives / (len(predictions) + len(ground_truths))

    # Calculate Average Precision (AP)
    average_precision = precision * recall

    return average_precision

from collections import defaultdict
from torch.optim.lr_scheduler import StepLR
import torch.optim as optim


# Lists to store values for plotting
bbox_losses = []
avg_ious = []
mAPs = []


freeze_layers = ['patchify', 'pos_encoding', 'transformer', 'layer_class1', 'sigmoid']
for name, param in model.named_parameters():
    if any(freeze_layer in name for freeze_layer in freeze_layers):
        param.requires_grad = False

# Allow the parameters of the bounding box layer to be trainable
for param in model.bbox_layer.parameters():
    param.requires_grad = True

# Define your loss functions and optimizer
criterion_bbox = nn.MSELoss()
optimizer = optim.Adam([
    {'params': model.bbox_layer.parameters()}  # Updated to bbox_layer instead of separate layers
], lr=0.0001)

# Learning rate scheduler
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

# Training loop
num_epochs = 25
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

for epoch in range(num_epochs):
    model.train()
    total_bbox_loss = 0.0
    total_iou = 0.0
    predictions = defaultdict(list)  # To store predicted bounding boxes for mAP calculation
    ground_truths = defaultdict(list)  # To store ground truth bounding boxes for mAP calculation

    for images, _, bboxes, _ in train_loader:
        images = images.to(device)
        bboxes = bboxes.to(device)

        optimizer.zero_grad()

        # Forward pass through the ViTObjectDetection model
        bbox_output, orig_output = model(images)

        # Compute the bounding box loss
        bbox_loss = criterion_bbox(bbox_output, bboxes)
        total_bbox_loss += bbox_loss.item()

        # Backward pass and optimization
        bbox_loss.backward()
        optimizer.step()

        # Extract predictions and ground truths for mAP calculation
        for i in range(images.size(0)):
            pred_bbox = bbox_output[i].detach().cpu().numpy()
            true_bbox = bboxes[i].detach().cpu().numpy()

            iou = calculate_iou(pred_bbox, true_bbox)
            total_iou += iou

            predictions[epoch].append({
                'class': 0,
                'bbox': pred_bbox,
                'score': 1.0
            })
            ground_truths[epoch].append({
                'class': 0,
                'bbox': true_bbox
            })

    avg_bbox_loss = total_bbox_loss / len(train_loader)
    avg_iou = total_iou / len(train_loader)

    # Calculate mAP
    iou_threshold = 0.5
    mAP = calculate_map(ground_truths[epoch], predictions[epoch], iou_threshold)

    # Append values for plotting
    bbox_losses.append(avg_bbox_loss)
    avg_ious.append(avg_iou)
    mAPs.append(mAP)

    # Print and plot results every few epochs (adjust as needed)
    if (1==1):
      print(f'Epoch {epoch + 1}/{num_epochs}, Bbox Loss: {avg_bbox_loss}, Avg IoU: {avg_iou}, mAP: {mAP}')

# Plot the loss, average IoU, and mAP
plt.figure(figsize=(12, 4))

# Plot the loss
plt.subplot(1, 3, 1)
plt.plot(range(1, num_epochs + 1), bbox_losses, label='Bbox Loss')
plt.title('Bounding Box Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot the average IoU
plt.subplot(1, 3, 2)
plt.plot(range(1, num_epochs + 1), avg_ious, label='Avg IoU')
plt.title('Average IoU')
plt.xlabel('Epoch')
plt.ylabel('IoU')
plt.legend()

# Plot the mAP
plt.subplot(1, 3, 3)
plt.plot(range(1, num_epochs + 1), mAPs, label='mAP')
plt.title('Mean Average Precision (mAP)')
plt.xlabel('Epoch')
plt.ylabel('mAP')
plt.legend()

# Show the plots
plt.tight_layout()
plt.show()

# After training, you can save or use the trained detector part of the model
torch.save(model.state_dict(), 'model_detector.pth')

import matplotlib.patches as patches

# Path to the validation images and labels
val_data_dir = '/content/drive/MyDrive/waldo/val'

model.eval()  # Set the model to evaluation mode

total_val_bbox_loss = 0.0
total_val_iou = 0.0
val_predictions = defaultdict(list)
val_ground_truths = defaultdict(list)

with torch.no_grad():  # No need to compute gradients during validation
    for val_images, _, val_bboxes, val_image_ids in val_loader:
        val_images = val_images.to(device)
        val_bboxes = val_bboxes.to(device)

        # Forward pass through the ViTObjectDetection model
        val_bbox_output, val_orig_output = model(val_images)

        # Compute the bounding box loss for validation
        val_bbox_loss = criterion_bbox(val_bbox_output, val_bboxes)
        total_val_bbox_loss += val_bbox_loss.item()

        # Extract predictions and ground truths for mAP calculation
        for i in range(val_images.size(0)):
            val_pred_bbox = val_bbox_output[i].detach().cpu().numpy()
            val_true_bbox = val_bboxes[i].detach().cpu().numpy()
            val_image_id = val_image_ids[i]

            val_iou = calculate_iou(val_pred_bbox, val_true_bbox)
            total_val_iou += val_iou

            val_predictions['val'].append({
                'class': 0,
                'bbox': val_pred_bbox,
                'score': 1.0
            })
            val_ground_truths['val'].append({
                'class': 0,
                'bbox': val_true_bbox
            })

            # Visualize the image with bounding boxes
            image_path = os.path.join(val_data_dir, 'img', f"{val_image_id}.jpg")
            image = Image.open(image_path)

            fig, ax = plt.subplots(1)
            ax.imshow(image)

            # Plot actual bounding box in green
            rect_true = patches.Rectangle(
                (val_true_bbox[0], val_true_bbox[1]),
                val_true_bbox[2],
                val_true_bbox[3],
                linewidth=2, edgecolor='g', facecolor='none'
            )
            ax.add_patch(rect_true)

            # Plot predicted bounding box in red
            rect_pred = patches.Rectangle(
                (val_pred_bbox[0], val_pred_bbox[1]),
                val_pred_bbox[2],
                val_pred_bbox[3],
                linewidth=2, edgecolor='r', facecolor='none'
            )
            ax.add_patch(rect_pred)

            plt.title(f'Image ID: {val_image_id}, IoU: {val_iou}')
            plt.show()

# Calculate average validation metrics
avg_val_bbox_loss = total_val_bbox_loss / len(val_loader)
avg_val_iou = total_val_iou / len(val_loader)

# Calculate mAP for validation
val_iou_threshold = 0.5  n
val_mAP = calculate_map(val_ground_truths['val'], val_predictions['val'], val_iou_threshold)

print(f'Validation Results - Bbox Loss: {avg_val_bbox_loss}, Avg IoU: {avg_val_iou}, mAP: {val_mAP}')